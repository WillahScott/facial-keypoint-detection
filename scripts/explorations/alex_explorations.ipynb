{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alex's explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gaussian Blur\n",
    "Let's modify the Guassian blur from the first problem set. This may help reduce some of the noise in the data. We especially work on the efficiency of the function from the first problem set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GaussianWeight(x, y, sigma = 1.5):\n",
    "    \"function that calculates the weights of neighboring pixels using a 2-D normal distribution\"\n",
    "    \n",
    "    # calculate the 2 halves of the formula separately for readability\n",
    "    first_half = 1/(2 * np.power(np.pi,2))\n",
    "    second_half = np.exp(-(np.power(x,2) + np.power(y,2))/(2 * np.power(sigma,2)))\n",
    "    \n",
    "    # bring the two-halves together\n",
    "    weight = first_half * second_half\n",
    "    \n",
    "    return weight\n",
    "\n",
    "def GaussianBlur(array, pic_dimension):\n",
    "    \"function that takes the weights the value of a individual pixel by the 8 neighboring pixels\"\n",
    "    \n",
    "    # source for Gaussian Blur method:\n",
    "    # http://www.pixelstech.net/article/1353768112-Gaussian-Blur-Algorithm\n",
    "    \n",
    "    # create a new array to hold the blurred values\n",
    "    blurred = []\n",
    "    \n",
    "    # create an array to store the relative locations, we hard-code these values\n",
    "    # as specific to the 8 pixel neighbors\n",
    "    relative_locations = [[0,1],[-1,1],[1,1],[0,-1],[-1,-1],[1,-1],[-1,0],[1,0]]\n",
    "\n",
    "    # we find the weights for each of the relative locations\n",
    "    # create an array to hold the relative weights\n",
    "    relative_weights = []\n",
    "    \n",
    "    # loop through each of the relative locations, finding the appropriate weight\n",
    "    for relative_location in relative_locations:\n",
    "        \n",
    "        # append the Gaussian weight to the relative weights array\n",
    "        relative_weights.append(GaussianWeight(relative_location[0],relative_location[1]))\n",
    "    \n",
    "    # pull each data point in the array\n",
    "    for digit in array: \n",
    "        \n",
    "        # reshape the digit to be a numpy array using the constansts defined\n",
    "        # above for the size of the digit\n",
    "        digit = np.array(digit)\n",
    "        digit_reshaped = np.reshape(digit,(pic_dimension,pic_dimension))\n",
    "        \n",
    "        # create an array to hold the blurred digit outcome\n",
    "        digit_blurred = []\n",
    "    \n",
    "        # loop through every row of the array\n",
    "        for index_i, row in enumerate(digit_reshaped):\n",
    "            \n",
    "            # loop through every column of the row of the array\n",
    "            for index_j, column in enumerate(row):\n",
    "\n",
    "                # set the coordinates relative to the pixel in question\n",
    "                # for example, the pixel in question is at (0,0), but the \n",
    "                # the pixel just above would be (0,1) because we go up 1\n",
    "                # but not move horizontally\n",
    "\n",
    "                # set the coordinates of the pixel and weight\n",
    "                coor = index_i, index_j, GaussianWeight(0,0)\n",
    "\n",
    "                # create an array to hold the value of each pixel\n",
    "                coordinates = []\n",
    "\n",
    "                # create an array to hold the weights from the Gaussian function\n",
    "                Gaussian_weights = []   \n",
    "                \n",
    "                # first add the initial pixel that is being weighted to the first place in the\n",
    "                # array\n",
    "                coordinates.append(digit_reshaped[coor[0]][coor[1]])\n",
    "                Gaussian_weights.append(coor[2])\n",
    "\n",
    "                # loop through each of the relative locations\n",
    "                for index_k,location in enumerate(relative_locations):\n",
    "\n",
    "                    # calculate the new location for the neighboring pixel\n",
    "                    relative_i = index_i + location[0]\n",
    "                    relative_j = index_j + location[1]\n",
    "\n",
    "                    # attempt 2 things: (1) find the neighboring pixel's value and \n",
    "                    # (2) find that pixels Gaussian weight\n",
    "                    try:\n",
    "                        coordinates.append(digit_reshaped[relative_i][relative_j])\n",
    "                        Gaussian_weights.append(relative_weights[index_k])\n",
    "                    except Exception:\n",
    "                        pass               \n",
    "\n",
    "                # calculate the relative Gaussian weights\n",
    "                # find the sum of the Gaussian weights and create an array to store the relative weights\n",
    "                relative_weights_sum = sum(Gaussian_weights)\n",
    "                relative_Gaussian_weights = []\n",
    "\n",
    "                # loop through each of the weights, recalculating as relative to the other weights\n",
    "                for weight in Gaussian_weights:\n",
    "                    relative_Gaussian_weights.append(weight/relative_weights_sum)\n",
    "\n",
    "                # create a new array to store the relative weights multiplied by the pixel values\n",
    "                Gaussian_pixels = []\n",
    "\n",
    "                # multiply the relative Gaussian weights by each pixel value\n",
    "                for index, weight in enumerate(relative_Gaussian_weights):\n",
    "                    Gaussian_pixels.append(coordinates[index] * weight)\n",
    "\n",
    "                # recalculate the new pixel as the sum of all neighboring pixels, appropriately weighted\n",
    "                new_pixel = sum(Gaussian_pixels)\n",
    "\n",
    "                # append this new pixel value to blurred array\n",
    "                digit_blurred.append(new_pixel)\n",
    "                \n",
    "        # append the blurred digit to the blurred array\n",
    "        blurred.append(digit_blurred)\n",
    "            \n",
    "    # return the new blurred array\n",
    "    return blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 30\n",
      "Training dataset size:  (2140,)\n",
      "Test dataset size:  (1783,)\n"
     ]
    }
   ],
   "source": [
    "# we verified that this modified function worked using digit data, now let's try it with\n",
    "# our actual data\n",
    "\n",
    "# set up our tools\n",
    "import sys\n",
    "sys.path.append('/Users/Alex/Documents/Berkeley/1601Spring/W207/PS4/facial-keypoint-detection/scripts')\n",
    "\n",
    "# import submit module from our tools subfolder\n",
    "from tools import submit, getdata\n",
    "\n",
    "# import our libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "# load the data\n",
    "_loaded = getdata.load_data(0, test=True, nonas=True)\n",
    "\n",
    "FEATURES = _loaded['features']\n",
    "print 'Number of features:', len(FEATURES)\n",
    "\n",
    "train_data = _loaded['training']['data']\n",
    "train_labels = _loaded['training']['labels']\n",
    "print 'Training dataset size: ', train_data.shape\n",
    "\n",
    "test_data = _loaded['test']['data']\n",
    "print 'Test dataset size: ', test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done blurring the data\n"
     ]
    }
   ],
   "source": [
    "# dimensions of the image, 96x96\n",
    "PIC_DIM = 96\n",
    "\n",
    "# let's put our data through the Gaussian blur function\n",
    "test_blurred = GaussianBlur(train_data[:10].tolist(), PIC_DIM)\n",
    "print \"Done blurring the data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250,)\n",
      "(250, 30)\n",
      "Index([u'left_eye_center_x', u'left_eye_center_y', u'right_eye_center_x',\n",
      "       u'right_eye_center_y', u'left_eye_inner_corner_x',\n",
      "       u'left_eye_inner_corner_y', u'left_eye_outer_corner_x',\n",
      "       u'left_eye_outer_corner_y', u'right_eye_inner_corner_x',\n",
      "       u'right_eye_inner_corner_y', u'right_eye_outer_corner_x',\n",
      "       u'right_eye_outer_corner_y', u'left_eyebrow_inner_end_x',\n",
      "       u'left_eyebrow_inner_end_y', u'left_eyebrow_outer_end_x',\n",
      "       u'left_eyebrow_outer_end_y', u'right_eyebrow_inner_end_x',\n",
      "       u'right_eyebrow_inner_end_y', u'right_eyebrow_outer_end_x',\n",
      "       u'right_eyebrow_outer_end_y', u'nose_tip_x', u'nose_tip_y',\n",
      "       u'mouth_left_corner_x', u'mouth_left_corner_y', u'mouth_right_corner_x',\n",
      "       u'mouth_right_corner_y', u'mouth_center_top_lip_x',\n",
      "       u'mouth_center_top_lip_y', u'mouth_center_bottom_lip_x',\n",
      "       u'mouth_center_bottom_lip_y'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "mini_train_data = train_data[:250]\n",
    "mini_train_labels = train_labels[:250]\n",
    "print mini_train_data.shape\n",
    "print mini_train_labels.shape\n",
    "print FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alex/miniconda2/envs/fkd/lib/python2.7/site-packages/sklearn/cross_validation.py:516: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "### LOGISTIC REGRESSION FITTED WITH THE BEST 'C'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# create a list to hold logistic regressions\n",
    "logistics = []\n",
    "\n",
    "# initalize a potential set of reasonable C values\n",
    "Lparameters = {'C':[0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0, 100.0]}\n",
    "\n",
    "\n",
    "# loop through every facial feature\n",
    "for index,facial_feature in enumerate(FEATURES):\n",
    "\n",
    "    # initalize the logistic regression model\n",
    "    logistic = LogisticRegression()\n",
    "\n",
    "    # set the C search with the given C options and the logistic model\n",
    "    C_search = GridSearchCV(logistic,Lparameters)\n",
    "\n",
    "    # fit the Gridsearch model to the data\n",
    "    C_search.fit(mini_train_data.tolist(),mini_train_labels[:,index])\n",
    "\n",
    "    # find the best C parameter\n",
    "    best_C = C_search.best_params_\n",
    "\n",
    "    # initalize a model with the best C\n",
    "    logistic_optimal = LogisticRegression(C = best_C['C'])\n",
    "    logistic_optimal.fit(mini_train_data.tolist(),mini_train_labels[:,index])\n",
    "    \n",
    "    # create a tuple with the name of the feature and the model\n",
    "    appending = facial_feature, logistic_optimal\n",
    "    \n",
    "    # append the name and the model to our list of facial feature models\n",
    "    logistics.append(appending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the logistic regression model to predict the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a multinomial naive bayes model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## NAIVE BAYES WITH OPTIMAL ALPHA\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# create an array to store the multinomial naive bayes models\n",
    "multinomials = []\n",
    "\n",
    "# initalize a set of reasonable alphas that we would like to search for the optimal alpha\n",
    "MNparameters = {'alpha':[0.001, 0.01, 0.1, 0.5, 1.0, 5.0, 10.0]}\n",
    "\n",
    "# loop through all the facial features\n",
    "for index,facial_feature in enumerate(FEATURES):\n",
    "    \n",
    "    # initalize the multinomail naive bayes model\n",
    "    Multinomial = MultinomialNB()\n",
    "\n",
    "    # set the alpha search with the given alpha options and the Multinomial model\n",
    "    alpha_search = GridSearchCV(Multinomial,MNparameters)\n",
    "\n",
    "    # fit the Gridsearch model on the training data\n",
    "    alpha_search.fit(train_data.tolist(),train_labels[:,index])\n",
    "\n",
    "    # find the best parameter\n",
    "    best_alpha = alpha_search.best_params_\n",
    "\n",
    "    # fit a model with the best alpha\n",
    "    Multinomial_optimal = MultinomialNB(alpha = best_alpha['alpha'])\n",
    "    Multinomial_optimal.fit(train_data.tolist(),train_labels[:,index])\n",
    "    \n",
    "    # create a tuple with the model and its associated facial feature\n",
    "    appending = facial_feature, Multinomial_optimal\n",
    "    \n",
    "    # append the model and its name to our list\n",
    "    multinomials.append(appending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Use the multinomial naive bayes model to predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predicting \"left_eye_center_x\"..."
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3b951dc60bc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msubmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultinomials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'multinomials'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Alex/Documents/Berkeley/1601Spring/W207/PS4/facial-keypoint-detection/scripts/tools/submit.pyc\u001b[0m in \u001b[0;36mcreate_generate\u001b[0;34m(test, models, label, verbose)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mpredicted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_submission\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# create the csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Alex/Documents/Berkeley/1601Spring/W207/PS4/facial-keypoint-detection/scripts/tools/submit.pyc\u001b[0m in \u001b[0;36mcreate_submission\u001b[0;34m(test, models, label, verbose)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0m_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# start timer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mpredicted_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0m_elapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0m_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not tuple"
     ]
    }
   ],
   "source": [
    "submit.create_generate(test_data.tolist(), multinomials, 'multinomials', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train a decision tree model on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unbound method fit() must be called with DecisionTreeClassifier instance as first argument (got list instance instead)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c26ff0e9532a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# fit a decision tree on the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdecisiontree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# create a tuple with the model and its associated facial feature name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unbound method fit() must be called with DecisionTreeClassifier instance as first argument (got list instance instead)"
     ]
    }
   ],
   "source": [
    "# import the decision tree libraries\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create an array to hold all the decision trees\n",
    "decisiontrees = []\n",
    "\n",
    "# loop through the facial features\n",
    "for index,facial_feature in enumerate(FEATURES):\n",
    "    \n",
    "    # fit a decision tree on the training data\n",
    "    decisiontree = DecisionTreeClassifier.fit(train_data.tolist(),train_labels[:,index])\n",
    "    \n",
    "    # create a tuple with the model and its associated facial feature name\n",
    "    appending = facial_feature, decisiontree\n",
    "    \n",
    "    # append the model to the array of decision trees\n",
    "    decisiontrees.append(decisiontree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the decision tree model to predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fit the random forest model on the training data\n",
    "This classifier is quite cool because it fits a number of different decision trees on subsets of the data and averages over all these samples to improve the accuracy of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unbound method fit() must be called with RandomForestClassifier instance as first argument (got list instance instead)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fdb0921f923e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# fit the random forest meta model to the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mrandomforest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# create a tuple with the model and its associated facial feature name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unbound method fit() must be called with RandomForestClassifier instance as first argument (got list instance instead)"
     ]
    }
   ],
   "source": [
    "# import the library\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# create an array to hold the random forest models\n",
    "randomforests = []\n",
    "\n",
    "# loop through the facial features\n",
    "for index,facial_feature in enumerate(FEATURES):\n",
    "    \n",
    "    # fit the random forest meta model to the data\n",
    "    randomforest = RandomForestClassifier.fit(train_data.tolist(),train_labels[:,index])\n",
    "    \n",
    "    # create a tuple with the model and its associated facial feature name\n",
    "    appending = facial_feature, randomforest\n",
    "    \n",
    "    # append the model and its name to the array of random forests\n",
    "    randomforests.append(appending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Use the random forests to make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Alex/miniconda2/envs/fkd/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "### RUN SOME TESTS USING DATA WE ALREADY KNOW WORKS\n",
    "\n",
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# Import a bunch of libraries.\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (70000, 784)\n",
      "label shape: (70000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the digit data either from mldata.org, or once downloaded to data_home, from disk. The data is about 53MB \n",
    "# so this cell should take a while the first time your run it.\n",
    "mnist = fetch_mldata('MNIST original', data_home='~/datasets/mnist')\n",
    "X, Y = mnist.data, mnist.target\n",
    "\n",
    "# Rescale grayscale values to [0,1].\n",
    "X = X / 255.0\n",
    "\n",
    "# Shuffle the input: create a random permutation of the integers between 0 and the number of data points and \n",
    "# apply this permutation to X and Y.\n",
    "# NOTE: Each time you run this cell, you'll re-shuffle the data, resulting in a different ordering.\n",
    "shuffle = np.random.permutation(np.arange(X.shape[0]))\n",
    "X, Y = X[shuffle], Y[shuffle]\n",
    "\n",
    "print 'data shape: ', X.shape\n",
    "print 'label shape:', Y.shape\n",
    "\n",
    "# Set some variables to hold test, dev, and training data.\n",
    "test_data, test_labels = X[61000:], Y[61000:]\n",
    "dev_data, dev_labels = X[60000:61000], Y[60000:61000]\n",
    "train_data, train_labels = X[:60000], Y[:60000]\n",
    "mini_train_data, mini_train_labels = X[:1000], Y[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# set up some real mini data\n",
    "real_data, real_labels = test_data[:10], test_labels[:10]\n",
    "\n",
    "# blur some data\n",
    "real_blurred = GaussianBlur(real_data, 28)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
